Tame Clear Buffalo

High

# x/batching EndBlock may leave some DataResult marked as "batched" without including them in any batch

### Summary

x/batching EndBlock may leave some DataResult marked as "batched" without including them in any batch

### Root Cause

Cosmos SDK requires state-changing functions to take a `ctx sdk.Context` param which, at the end of the block, will be always committed even if there is an error/panic.

In this case, the x/batching EndBlock uses the `defer()/recover()` pattern to prevent the chain from halting, which is good.

However, in case the `defer()` block is entered, any state written on `ctx` prior to that point will be committed anyway.

In particular, inside [ConstructDataResultTree()](https://github.com/sherlock-audit/2024-12-seda-protocol/blob/051b5e88a2f530792913910ebf98c50f431b1e3b/seda-chain/x/batching/keeper/endblock.go#L145), the "unbatched" data results are marked as "batched" 

```js
err = k.MarkDataResultAsBatched(ctx, res, newBatchNum)
if err != nil {
    return types.DataResultTreeEntries{}, nil, err
}
```

So, if any errors after such marking happens before the end of the EndBlock function, a batch can result "batched" without being actually included in the real batch.

### Internal Pre-conditions

- any error after ConstructDataResultTree() or a single MarkDataResultAsBatched()

### External Pre-conditions

### Attack Path

### Impact

Some DataResult will be marked as "batched" without being included in any batch if there is any error after [ConstructDataResultTree()](https://github.com/sherlock-audit/2024-12-seda-protocol/blob/051b5e88a2f530792913910ebf98c50f431b1e3b/seda-chain/x/batching/keeper/endblock.go#L145) mentioned in the Root Cause section.

### PoC

We're going to simulate a panic after a test data result is marked as batched, start by applying the following modifications to the chain's code:

x/pubkey/types/params.go
```diff
const (
+	DefaultActivationBlockDelay             = 8       // to make the proving scheme active at height 9
	DefaultActivationThresholdPercent = 80
)
```

x/batching/keeper/data_result.go
```diff
import (
+    sdk "github.com/cosmos/cosmos-sdk/types"
)

func (k Keeper) GetDataResults(ctx context.Context, batched bool) ([]types.DataResult, error) {

+	if sdk.UnwrapSDKContext(ctx).BlockHeight() == 9 {
+		testResult := types.DataResult{
+			Id:             "746573742d6964",       // "test-id" in hex
+			DrId:           "746573742d64722d6964", // "test-dr-id" in hex
+			DrBlockHeight:  9,
+			Version:        "v1.0.0",
+			BlockHeight:    9,
+			BlockTimestamp: 1700000000, // Example timestamp
+			ExitCode:       0,
+			GasUsed:        nil, // Adjust as needed
+			Result:         []byte("test-result"),
+			PaybackAddress: "cosmos1testaddress",
+			SedaPayload:    "test-payload",
+			Consensus:      true,
+		}
+		return []types.DataResult{testResult}, nil
+	}

	var results []types.DataResult
	err := k.IterateDataResults(ctx, batched, func(_ collections.Triple[bool, string, uint64], value types.DataResult) (bool, error) {
		results = append(results, value)
		return false, nil
	})
	return results, err
}
```

x/batching/keeper/endblock.go:ConstructDataResult()
```diff
func (k Keeper) ConstructDataResultTree(ctx sdk.Context, newBatchNum uint64) (types.DataResultTreeEntries, []byte, error) {
	dataResults, err := k.GetDataResults(ctx, false)
	if err != nil {
		return types.DataResultTreeEntries{}, nil, err
	}

	entries := make([][]byte, len(dataResults))
	treeEntries := make([][]byte, len(dataResults))
	for i, res := range dataResults {
		resID, err := hex.DecodeString(res.Id) 
		if err != nil {
			return types.DataResultTreeEntries{}, nil, err
		}
		entries[i] = resID
		treeEntries[i] = append([]byte{utils.SEDASeparatorDataResult}, resID...)

		// @audit res is marked as batched to avoid inclusion in later batches
		err = k.MarkDataResultAsBatched(ctx, res, newBatchNum)
		if err != nil {
			return types.DataResultTreeEntries{}, nil, err
		}

+		if ctx.BlockHeight() == 9 { // POC
+			ctx.Logger().Warn("[POC] AFTER data result marked as batched")
+			panic("AFTER MARKDATARESULTASBATHCED")
+		}
+	}

	return types.DataResultTreeEntries{Entries: entries}, utils.RootFromEntries(treeEntries), nil
}
```

Then launch the local_multi_setup.sh testing script and wait for the panic at height 9 to happen.

Now run the following command to query the test data result:
```bash
sedad query batching data-result 746573742d64722d6964
```

As you can see it is marked as included in the batch number 1

```bash
$ sedad query batching data-result 746573742d64722d6964
batch_assignment:
  batch_number: "1"
  data_request_height: "9"
  data_request_id: 746573742d64722d6964
data_result:
  block_height: "9"
  block_timestamp: "1700000000"
  consensus: true
  dr_block_height: "9"
  dr_id: 746573742d64722d6964
  exit_code: 0
  gas_used: null
  id: 746573742d6964
  payback_address: cosmos1testaddress
  result: dGVzdC1yZXN1bHQ=
  seda_payload: test-payload
  version: v1.0.0
```

However, such batch doesnÂ´t even exist because the panic happened before `k.SetNewBatch()`:

```bash
$ sedad query batching batches
batches: []
pagination:
  next_key: null
  total: "0"
```

At the next block, `k.GetDataResults(ctx, false)`, will not return this test batch because it is marked as batched.

### Mitigation

Wrap the entire batch construction process in a temporary `CacheContext` which, if not explicitly written via `writeCache()`, will discard any changes in case of errors or panics.

```diff
func (k Keeper) EndBlock(ctx sdk.Context) (err error) {
	// Use defer to prevent returning an error, which would cause the chain to halt.
	defer func() {
		// Handle a panic.
		if r := recover(); r != nil {
			k.Logger(ctx).Error("recovered from panic in batching end blocker", "err", r)
		}
		// Handle an error.
		if err != nil {
			k.Logger(ctx).Error("error in batching end blocker", "err", err)
		}
		err = nil
	}()

	isActivated, err := k.pubKeyKeeper.IsProvingSchemeActivated(ctx, utils.SEDAKeyIndexSecp256k1)
	if err != nil {
		return err
	}
	if !isActivated {
		k.Logger(ctx).Warn("x/batching proving scheme not activated")
		return nil
	}

+	cc, writeCache := ctx.CacheContext() // FIX

-	batch, dataEntries, valEntries, err := k.ConstructBatch(ctx)
+	batch, dataEntries, valEntries, err := k.ConstructBatch(cc)
	if err != nil {
		if errors.Is(err, types.ErrNoBatchingUpdate) {
			k.Logger(ctx).Info(fmt.Sprintf("skip batch creation for height %d", ctx.BlockHeight()))
			return nil
		}
		return err
	}

-	err = k.SetNewBatch(ctx, batch, dataEntries, valEntries)
+	err = k.SetNewBatch(cc, batch, dataEntries, valEntries)
	if err != nil {
		return err
	}

+      writeCache()

	return nil
}
```
